{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80db15beaa332df4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_feature_extraction_files(directory):\n",
    "    \"\"\"\n",
    "    Searches for 'feature_extraction.csv' files within each subdirectory of a given directory.\n",
    "    \n",
    "    Args:\n",
    "    directory (str): The path to the directory to search within.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of full paths to each 'feature_extraction.csv' file found.\n",
    "    \"\"\"\n",
    "    csv_files = []\n",
    "    # Walk through each subdirectory in the provided directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # Check if 'feature_extraction.csv' is in the files list\n",
    "        if 'feature_extraction.csv' in files:\n",
    "            # Construct full path and add to the list\n",
    "            full_path = os.path.join(root, 'feature_extraction.csv')\n",
    "            csv_files.append(full_path)\n",
    "            \n",
    "    return csv_files\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28e9436a6188da87",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_csv(data_file_path):\n",
    "    \n",
    "    df_read_file = pd.read_csv(data_file_path)\n",
    "    df_Acc = df_read_file.loc[:, :\"Label\"]\n",
    "    X_data = df_Acc.to_numpy()\n",
    "    \n",
    "    y_data = df_read_file.loc[:,\"Label\"]\n",
    "    y_data = np.around(y_data, decimals=0)\n",
    "\n",
    "    return X_data, y_data\n",
    "\n",
    "def normalize_positions(data):\n",
    "    posX_columns = [0] + list(range(4, 22))   # Adjusted for 0-based indexing: Columns 0 and 4 to 21 for PosX normalization\n",
    "    posY_columns = [1] + list(range(22, 39))  # Adjusted for 0-based indexing: Columns 1 and 22 to 38 for PosY normalization\n",
    "    \n",
    "    # Normalize PosX columns\n",
    "    data[:, posX_columns] = data[:, posX_columns] / 1920\n",
    "    # Normalize PosY columns\n",
    "    data[:, posY_columns] = data[:, posY_columns] / 1080\n",
    "            \n",
    "    return data\n",
    "\n",
    "def handle_indices(data, indexes_of_labels, label_value=0, label_col=0):\n",
    "    # Ensure data is a single column NumPy array\n",
    "    data = data[:, label_col] if data.ndim > 1 else data\n",
    "    \n",
    "\n",
    "    found_indexes = set(indexes_of_labels)  # Using a set to avoid duplicates\n",
    "\n",
    "    if indexes_of_labels:\n",
    "        # Handling '0' label exactly 3 lines before the first label\n",
    "        first_label_index = indexes_of_labels[0]\n",
    "        target_index_before_first = first_label_index - 3\n",
    "        if 0 <= target_index_before_first < len(data) and data[target_index_before_first] == label_value:\n",
    "            found_indexes.add(target_index_before_first)\n",
    "\n",
    "        # Handling '0' label exactly 3 lines after the last label\n",
    "        last_label_index = indexes_of_labels[-1]\n",
    "        target_index_after_last = last_label_index + 3\n",
    "        if 0 <= target_index_after_last < len(data) and data[target_index_after_last] == label_value:\n",
    "            found_indexes.add(target_index_after_last)\n",
    "\n",
    "        # Handling the '0' label in the middle between given label indices\n",
    "        for i in range(len(indexes_of_labels) - 1):\n",
    "            start_index = indexes_of_labels[i]\n",
    "            end_index = indexes_of_labels[i + 1]\n",
    "            middle_index = (start_index + end_index) // 2\n",
    "            if data[middle_index] == label_value:\n",
    "                found_indexes.add(middle_index)\n",
    "\n",
    "    # Convert set to a sorted list\n",
    "    sorted_indexes = sorted(found_indexes)\n",
    "    \n",
    "    return sorted_indexes\n",
    "\n",
    "\n",
    "def lstm_data_transform(x_data, y_data, keys, num_steps=6):\n",
    "    X, y = list(), list()\n",
    "    \n",
    "    # Loop over the provided end indexes\n",
    "    for end_ix in keys:\n",
    "        # Calculate the start index for the current window\n",
    "        start_ix = end_ix - num_steps + 1\n",
    "        \n",
    "        # Ensure the start index is not negative\n",
    "        if start_ix < 0:\n",
    "            continue\n",
    "        \n",
    "        # Get a sequence of data for x\n",
    "        seq_X = x_data[start_ix:end_ix + 1, :-1]\n",
    "        \n",
    "        # Check if the sequence has the correct number of steps\n",
    "        if seq_X.shape[0] == num_steps:\n",
    "\n",
    "            # Append the sequence and target to the lists\n",
    "            X.append(seq_X)\n",
    "    \n",
    "    # Convert the lists to numpy arrays\n",
    "    x_array = np.array(X)\n",
    "    y_array = np.array([0, 1, 0, 2, 0, 3, 0, 4, 0, 5, 0])\n",
    "    \n",
    "    return x_array, y_array"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da493681d88ab4cd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_relevant_rows(csv_path):\n",
    "    x_array, y_array = read_csv(csv_path)\n",
    "    \n",
    "    # Normalize the position columns\n",
    "    x_array = normalize_positions(x_array)\n",
    "\n",
    "    # Define the values to check\n",
    "    values_to_check = {1, 2, 3, 4, 5}\n",
    "\n",
    "    # Find the indexes of values in y_array that are in values_to_check\n",
    "    indexes_of_labels = [index for index, value in enumerate(y_array) if value in values_to_check]\n",
    "\n",
    "    sorted_indices = handle_indices(y_array, indexes_of_labels)\n",
    "    \n",
    "    # print('Sorted indices: ', sorted_indices)\n",
    "    \n",
    "    if len(sorted_indices) <= 10:\n",
    "        \n",
    "        print(csv_path)\n",
    "\n",
    "    x_data, y_data = lstm_data_transform(x_array, y_array, sorted_indices, num_steps=6)\n",
    "\n",
    "    return x_data, y_data\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "578cb930756e4b7b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Example of how to call this function for one CSV file\n",
    "file_paths = find_feature_extraction_files('Frames')\n",
    "\n",
    "print(len(file_paths))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62f0c3905190c28f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_of_steps = 6\n",
    "num_of_input_signals = 55\n",
    "\n",
    "X_data = np.empty([0, num_of_steps, num_of_input_signals], dtype=\"float32\")\n",
    "y_data = np.empty([0], dtype=\"float32\")\n",
    "\n",
    "for file_path in file_paths:\n",
    "    \n",
    "    data, labels = extract_relevant_rows(file_path)\n",
    "    \n",
    "    # print(labels.shape)\n",
    "    \n",
    "    X_data = np.append(X_data, data[:], axis=0)\n",
    "    \n",
    "    # print(labels.shape)\n",
    "    y_data = np.append(y_data, labels, axis=0)\n",
    "    \n",
    "print(X_data.shape)   # This should show (x, 6, 55) if  everything is correctly configured\n",
    "print(y_data.shape)  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb4673351a70bbe6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(y_data):\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_data_encoded = encoder.fit_transform(y_data)\n",
    "\n",
    "    return y_data_encoded"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8e58440bc43804c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # # One-hot encode the labels\n",
    "# final_labels_encoded = one_hot_encode_labels(y_data.reshape(-1,1))\n",
    "\n",
    "final_labels_encoded = keras.utils.to_categorical(y_data, num_classes=6)\n",
    "\n",
    "print(final_labels_encoded)\n",
    "\n",
    "# print('x_data shape:', final_data.shape)\n",
    "print('y_data shape:', final_labels_encoded.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf8044007422d6cc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# splits = custom_k_fold_split(x_data_flat, y_data_flat, n_splits=5, window_size=num_of_steps)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=42)\n",
    "    \n",
    "for train_index, test_index in sss.split(X_data, y_data): \n",
    "    X_train_pre, X_test = X_data[train_index], X_data[test_index] \n",
    "    y_train_pre, y_test = final_labels_encoded[train_index], final_labels_encoded[test_index] \n",
    "    \n",
    "for train_index, val_index in sss.split(X_train_pre, y_train_pre):\n",
    "    X_train, X_val = X_train_pre[train_index], X_train_pre[val_index] \n",
    "    y_train, y_val = y_train_pre[train_index], y_train_pre[val_index] \n",
    "\n",
    "# y_test.resize(y_test.shape[0],1,y_test.shape[1])\n",
    "# y_train.resize(y_train.shape[0],1, y_train.shape[1])\n",
    "# y_val.resize(y_val.shape[0],1, y_val.shape[1])\n",
    "    \n",
    "print(\"Train data shape:\", X_train.shape)\n",
    "print(\"Train labels shape:\", y_train.shape)\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Validation data shape:\", X_val.shape)\n",
    "print(\"Validation labels shape:\", y_val.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd4990ecf7cc8cf8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, RMSprop \n",
    "\n",
    "from tensorflow.keras.regularizers import l1_l2, l2\n",
    "\n",
    "########################################################\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Dense, InputLayer, Reshape, TimeDistributed, Dropout, BatchNormalization, Bidirectional\n",
    "\n",
    "# Define the CNN-LSTM model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Input layer - assumes input_shape is defined appropriately for your data\n",
    "# For example, input_shape could be (sequence_length, features_per_step)\n",
    "model.add(InputLayer(shape=(6, 55)))\n",
    "# Batch Normalization layer before LSTM to normalize inputs\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding a Bidirectional LSTM layer with L2 regularization\n",
    "# Note: Adjust the `l2` regularization strength as needed\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=l1_l2(l1=1e-5, l2=0.001))))\n",
    "# Adding another Bidirectional LSTM layer\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False, dropout=0.3,recurrent_dropout=0.3)))\n",
    "\n",
    "# Output layer - assuming a classification problem with 6 classes\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# model.add(InputLayer(shape=(6, 55)))\n",
    "# model.add(Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))\n",
    "# model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))\n",
    "# \n",
    "# # Ensure the kernel size is smaller than the sequence length\n",
    "# model.add(Conv1D(32, 5, activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=1))  # Reduce the sequence length by a factor of 2\n",
    "# model.add(Conv1D(16, 2, activation='relu'))\n",
    "# \n",
    "# # Since MaxPooling1D and Conv1D layers have reduced the sequence length, we need to adjust for the LSTM layer\n",
    "# model.add(LSTM(16, activation='tanh', return_sequences=True))\n",
    "# model.add(Dropout(0.5))\n",
    "# \n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(6, activation='softmax'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60465be6f3b2ac2a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98b4440b35fecb69",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "999a2fb827c2b148",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f6dfd6ee894f299",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from keras.src.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "es_callback = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.15, patience=15)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a0b94183492e344",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.benchmark import timer\n",
    "\n",
    "# fit the keras model on the dataset0\n",
    "startTime = timer()\n",
    "history = model.fit(X_train, y_train, epochs=500, callbacks=[es_callback, reduce_lr], validation_data=(X_val, y_val), batch_size=128, verbose=1)#, class_weight=class_weight_dict)\n",
    "\n",
    "endTime = timer()\n",
    "print(\"Model trained in {:f}s.\".format(endTime - startTime))\n",
    "print(\"This is {:f}min.\".format((endTime - startTime)/60))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c838c202efbda00f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_test and y_test are already defined and preprocessed\n",
    "# Predict on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print the shapes to debug\n",
    "print(f\"Shape of predictions: {predictions.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "# # Reshape the predictions and y_test to remove the extra dimension\n",
    "# predictions = predictions.reshape(predictions.shape[0], predictions.shape[2])\n",
    "# y_test = y_test.reshape(y_test.shape[0], y_test.shape[2])\n",
    "\n",
    "# Print the shapes to debugs\n",
    "print(f\"Shape of predictions after reshaping: {predictions.shape}\")\n",
    "print(f\"Shape of y_test after reshaping: {y_test.shape}\")\n",
    "\n",
    "# Convert predictions and true labels to class indices\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print the shapes to debug\n",
    "print(f\"Shape of predicted_classes: {predicted_classes.shape}\")\n",
    "print(f\"Shape of true_classes: {true_classes.shape}\")\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Define your custom string labels\n",
    "labels = ['Unclassified Movement', 'Initial Pose', 'Pole Plant', 'Push', 'Pole Release', 'Straightening']\n",
    "\n",
    "# Display the confusion matrix with string labels\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "# Display the confusion matrix\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "ax = disp.plot(cmap=plt.cm.Blues)\n",
    "# Remove grid lines\n",
    "ax.ax_.grid(False)  # Turn off the grid lines\n",
    "\n",
    "# Rotate x-axis labels to prevent overlap\n",
    "plt.xticks(rotation=45)\n",
    "# Optionally, adjust font size\n",
    "plt.gcf().autofmt_xdate()  # Adjust layout to make room for diagonal labels\n",
    "plt.xlabel('Predicted Label', fontsize=16)\n",
    "plt.ylabel('True Label', fontsize=16)\n",
    "\n",
    "# Save the figure as an EPS file\n",
    "# plt.savefig('confusion_matrix.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4882a101ec9de4f2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Generate the normalized confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes, normalize='true')\n",
    "\n",
    "# Define your custom string labels\n",
    "labels = ['Unclassified Movement', 'Initial Pose', 'Pole Plant', 'Push', 'Pole Release', 'Straightening']\n",
    "\n",
    "# Display the confusion matrix with string labels\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "# Display the confusion matrix\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "ax = disp.plot(cmap=plt.cm.Blues)\n",
    "# Remove grid lines\n",
    "ax.ax_.grid(False)  # Turn off the grid lines\n",
    "\n",
    "# Rotate x-axis labels to prevent overlap\n",
    "plt.xticks(rotation=45)\n",
    "# Optionally, adjust font size\n",
    "plt.gcf().autofmt_xdate()  # Adjust layout to make room for diagonal labels\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "\n",
    "# Save the figure as an EPS file\n",
    "plt.savefig('confusion_matrix_normalized_aug_4_128.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "321f1d9286e9eb58",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate overall accuracy from the confusion matrix\n",
    "overall_accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"Overall Accuracy: {overall_accuracy: .2f}%\")\n",
    "\n",
    "# Generate the raw confusion matrix (for accuracy calculation)\n",
    "cm_raw = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Calculate overall accuracy from the raw confusion matrix\n",
    "overall_accuracy = np.trace(cm_raw) / np.sum(cm_raw)\n",
    "print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca769443f3e2178a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'history' is the output from your model.fit() call\n",
    "# Extracting the data from the history object\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "accuracy = history.history.get('accuracy')\n",
    "val_accuracy = history.history.get('val_accuracy')\n",
    "\n",
    "# Creating the plots\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Assuming 'epochs', 'loss', 'val_loss', 'accuracy', and 'val_accuracy' are defined\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting training and validation loss in a separate figure\n",
    "plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "#plt.title('Training and Validation Loss')\n",
    "plt.legend(fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.grid(False)  # Turn off the grid\n",
    "plt.savefig('training_loss_aug_4_128.eps', format='eps', bbox_inches='tight')  # Save if needed\n",
    "plt.show()\n",
    "\n",
    "# Plotting training and validation accuracy in a separate figure\n",
    "if accuracy and val_accuracy:\n",
    "    plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
    "    plt.plot(epochs, accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs', fontsize=18)\n",
    "    plt.ylabel('Accuracy', fontsize=18)\n",
    "    # plt.title('Training and Validation Accuracy')\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.grid(False)  # Turn off the grid\n",
    "    plt.savefig('training_accuracy_aug_4_128.eps', format='eps', bbox_inches='tight')  # Save if needed\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "652e0a1f124c745c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2829e94b7f6d93ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
